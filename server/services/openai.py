from typing import *

from aiofauna import ApiClient
from pydantic import BaseModel
from pydantic import Field as Data

from ..config import env
from ..templates import Context, SystemMessageTemplate

Schema = Dict[str, Any]

class FunctionModel(BaseModel):
    @classmethod
    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if cls.__doc__ is None:
            raise TypeError(f"OpenAI FunctionSchema {cls.__name__} must have a description")
        json_schema = cls.schema()
        cls.function = {
            "name": json_schema["title"],
            "description": cls.__doc__,
            "parameters": {
                "type": json_schema["type"],
                "properties": json_schema["properties"],
                "required": json_schema["required"],
            },
        }

class ChatMessage(BaseModel):
    """
    ChatMessage
        - role: Literal["system", "user","assistant"]
        - content: str
    """

    role: Literal["system", "user", "assistant"] = Data(
        ..., description="Role of the message"
    )
    content: str = Data(..., description="Content of the message")


class ChatRequest(BaseModel):
    """
    ChatRequest
        - model: str
        - messages: List[ChatMessage]
        - temperature: float
        - max_tokens: int
        - top_p: float
        - frequency_penalty: float
        - presence_penalty: float
        - stop: List[str]
        - n: int
    """

    model: str = Data(
        default="gpt-3.5-turbo-16k-0613",
        description="The ID of the engine to use for completion.",
    )
    messages: List[ChatMessage] = Data(
        default=[], description="Pair of messages from system and human", max_items=2
    )
    temperature: float = Data(
        default=0.75,
        description="What we call the 'creativity' of the AI. 0.0 is very conservative (highly repetitive), 1.0 is very creative (may say strange things or diverge from the topic at hand).",
    )
    max_tokens: int = Data(
        default=2048,
        description="The maximum number of tokens to generate. Requests can use up to 2048 tokens shared between prompt and completion.",
    )
    top_p: float = Data(
        default=1.0,
        description="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
    )
    frequency_penalty: float = Data(
        default=0.25,
        description="What we call the 'creativity' of the AI. 0.0 is very conservative (highly repetitive), 1.0 is very creative (may say strange things or diverge from the topic at hand).",
    )
    presence_penalty: float = Data(
        default=0.25,
        description="What we call the 'creativity' of the AI. 0.0 is very conservative (highly repetitive), 1.0 is very creative (may say strange things or diverge from the topic at hand).",
    )
    n: int = Data(
        default=1, description="The number of completions to generate for each prompt."
    )

    def render(self, user: str, prompt: str, context: List[Context], role:str):
        self.messages = [
            ChatMessage(role="user", content=prompt),
            ChatMessage(
                role="assistant",
                content=SystemMessageTemplate(name=user, context=context, role=role).render(),
            ),
        ]
        return self


class Choice(BaseModel):
    """
    Choice
        - index: int
        - message: ChatMessage
        - finish_reason: str
    """

    index: int = Data(
        ..., description="The index of the choice that was selected by the model."
    )
    message: ChatMessage = Data(
        ..., description="The text of the choice that was selected by the model."
    )
    finish_reason: str = Data(
        ...,
        description="The reason the conversation ended. This will be completion, api_call, or timeout.",
    )


class ChatUsage(BaseModel):
    """
    ChatUsage
        - prompt_tokens: int
        - completion_tokens: int
        - total_tokens: int
    """

    prompt_tokens: int = Data(..., description="The number of tokens in the prompt.")
    completion_tokens: int = Data(
        ..., description="The number of tokens in the completion."
    )
    total_tokens: int = Data(
        ..., description="The total number of tokens generated by the engine."
    )


class ChatResponse(BaseModel):
    """
    ChatResponse
        - id: str
        - object: str
        - created: int
        - choices: List[Choice]
        - usage: ChatUsage
    """

    id: str = Data(..., description="The unique identifier for the completion.")
    object: str = Data(..., description="The string 'text_completion'.")
    created: int = Data(
        ..., description="When the prompt was created (UNIX timestamp)."
    )
    choices: List[Choice] = Data(
        ..., description="The list of choices the engine made, in chronological order."
    )
    usage: ChatUsage = Data(
        ...,
        description="The object containing information about the model's resource usage.",
    )


class ChatCompletionRequest(BaseModel):
    """
    ChatCompletionRequest
        - user: str
        - prompt: str
        - context: List[Context]
    """

    user: str = Data(..., description="The ref of the user")
    prompt: str = Data(..., description="The prompt to send to the AI")
    context: List[Context] = Data(..., description="The context to send to the AI")
    role: str = Data("assistant", description="The role that the chatbot should play")

class EmbeddingRequest(BaseModel):
    """
    EmbeddingRequest
        - model: str
        - input: str
    """

    model: str = Data(default="text-embedding-ada-002")
    input: str = Data(..., description="The text to embed")


class EmbeddingItem(BaseModel):
    """
    EmbeddingItem
        - object: str
        - index: int
        - embedding: List[float]
    """

    object: str = Data(..., description="The string 'embedding'.")
    index: int = Data(..., description="The index of the input.")
    embedding: List[float] = Data(..., description="The embedding of the input.")


class EmbeddingUsage(BaseModel):
    """
    EmbeddingUsage
        - prompt_tokens: int
        - total_tokens: int
    """

    prompt_tokens: int = Data(..., description="The number of tokens in the prompt.")
    total_tokens: int = Data(
        ..., description="The total number of tokens generated by the engine."
    )


class EmbeddingResponse(BaseModel):
    """
    EmbeddingResponse
        - object: str
        - data: List[EmbeddingItem]
        - model: str
        - usage: EmbeddingUsage
    """

    object: str = Data(..., description="The string 'list'.")
    data: List[EmbeddingItem] = Data(
        ..., description="The list of embeddings, in the same order as the input list."
    )
    model: str = Data(..., description="The model used to generate the embeddings.")
    usage: EmbeddingUsage = Data(
        ...,
        description="The object containing information about the model's resource usage.",
    )


class OpenAIService(ApiClient):
    """
    OpenAIService
        - get_completion(request:ChatCompletionRequest) -> ChatResponse
        - get_embeddings(request:EmbeddingRequest) -> EmbeddingResponse
    """

    base_url = "https://api.openai.com/v1"
    headers = {
        "Authorization": f"Bearer {env.OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    def __init__(self, *args, **kwargs):
        super().__init__(self.base_url, headers=self.headers, *args, **kwargs)

    async def get_completion(self, request: ChatCompletionRequest):
        response = await self.fetch(
            "/chat/completions",
            method="POST",
            json=ChatRequest().render(**request.dict()).dict(),
        )
        assert isinstance(response, dict)
        return ChatResponse(**response)

    async def get_embeddings(self, request: EmbeddingRequest):
        response = await self.fetch("/embeddings", method="POST", json=request.dict())
        assert isinstance(response, dict)
        return EmbeddingResponse(**response)